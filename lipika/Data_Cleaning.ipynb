{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from sklearn import preprocessing\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read in csv files\n",
    "data_2013 = pd.read_csv(\"../data/2012_13_loan_data/LoanStats3b.csv\", low_memory = False, encoding='latin-1')\n",
    "\n",
    "data_a_2014 = pd.read_csv(\"../data/2014_loan_data/LoanStats_2014_A.csv\", low_memory = False);\n",
    "data_b_2014 = pd.read_csv(\"../data/2014_loan_data/LoanStats_2014_B.csv\", low_memory = False);\n",
    "data_c_2014 = pd.read_csv(\"../data/2014_loan_data/LoanStats_2014_C.csv\", low_memory = False);\n",
    "data_d_2014 = pd.read_csv(\"../data/2014_loan_data/LoanStats_2014_D.csv\", low_memory = False);\n",
    "\n",
    "data_a_2015 = pd.read_csv(\"../data/2015_loan_data/LoanStats_2015_A.csv\", low_memory = False);\n",
    "data_b_2015 = pd.read_csv(\"../data/2015_loan_data/LoanStats_2015_B.csv\", low_memory = False);\n",
    "data_c_2015 = pd.read_csv(\"../data/2015_loan_data/LoanStats_2015_C.csv\", low_memory = False);\n",
    "data_d_2015 = pd.read_csv(\"../data/2015_loan_data/LoanStats_2015_D.csv\", low_memory = False);\n",
    "data_e_2015 = pd.read_csv(\"../data/2015_loan_data/LoanStats_2015_E.csv\", low_memory = False);\n",
    "data_f_2015 = pd.read_csv(\"../data/2015_loan_data/LoanStats_2015_F.csv\", low_memory = False);\n",
    "data_g_2015 = pd.read_csv(\"../data/2015_loan_data/LoanStats_2015_G.csv\", low_memory = False);\n",
    "data_h_2015 = pd.read_csv(\"../data/2015_loan_data/LoanStats_2015_H.csv\", low_memory = False);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## concatenate data into training set\n",
    "data_train = pd.concat([data_a_2014, data_b_2014, data_c_2014, data_d_2014, data_2013], ignore_index=True)\n",
    "\n",
    "data_2015 = pd.concat([data_a_2015, data_b_2015, data_c_2015, data_d_2015, \n",
    "                       data_e_2015, data_f_2015, data_g_2015, data_h_2015], \n",
    "                       ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## checking for policy code 2 in train and test\n",
    "assert sum(data_train.policy_code == 2) == 0\n",
    "assert sum(data_2015.policy_code == 2) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(data_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper functions\n",
    "def convert_dates(x):\n",
    "    \n",
    "    # Ignore dates with NaNs - will deal with it when taking difference\n",
    "    if pd.isnull(x) is False:\n",
    "        try:\n",
    "            x = datetime.datetime.strptime(x,'%b-%y')\n",
    "        except:\n",
    "            x = datetime.datetime.strptime(x,'%b-%Y')\n",
    "    return x\n",
    "\n",
    "def diff_year(x, d1, d2):\n",
    "    # Only change term for repaid loans\n",
    "    if x['loan_status'] in ['Fully Paid', 'paid']:\n",
    "        m = (d1.year - d2.year) + (d1.month - d2.month)/12\n",
    "        \n",
    "        # For loans that were fully paid back in the same month that they were issued,\n",
    "        # assume they were repaid in 15 days\n",
    "        if m == 0 and x['last_pymnt_amnt'] >= x['funded_amnt']:\n",
    "            m = 0.04\n",
    "\n",
    "    # For distressed loans, use original term in years\n",
    "    else:\n",
    "        m = 1/12 * x['terms_in_months']\n",
    "    return m\n",
    "\n",
    "def get_term_adj(df):\n",
    "    \n",
    "    # Apply date format\n",
    "    df['issue_d1'] = df['issue_d'].apply(lambda x: convert_dates(x))\n",
    "    df['last_pymnt_d1'] = df['last_pymnt_d'].apply(lambda x: convert_dates(x))\n",
    "    \n",
    "    # Get term in months\n",
    "    df['terms_in_months'] = df['term'].apply(lambda x: int(x[1:3]))\n",
    "    \n",
    "    # Apply difference\n",
    "    df['term_adj'] = df.apply(lambda x: diff_year(x, x['last_pymnt_d1'], x['issue_d1']), axis=1)\n",
    "#     df['term_adj'] = df['term_adj'].fillna((1/12)*df['terms_in_months'])\n",
    "    \n",
    "    # Drop helper cols\n",
    "    df.drop(['terms_in_months', 'issue_d1','last_pymnt_d1'], axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def format_revol(val):\n",
    "    percent = val[:-1]\n",
    "    return float(percent)/100\n",
    "\n",
    "def format_interest(val):\n",
    "    percent = val[:-1]\n",
    "    return float(percent)/100\n",
    "\n",
    "def format_term(val):\n",
    "    term = val[1:3]\n",
    "    return float(term)/12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_model(df, columns, columns_to_encode, columns_to_normalize):\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    new_df = df.copy()\n",
    "    new_df = new_df[columns]\n",
    "    new_df = pd.get_dummies(new_df, columns = columns_to_encode)\n",
    "    new_df['amnt'] = new_df['funded_amnt']\n",
    "    new_df[columns_to_normalize] = min_max_scaler.fit_transform(new_df[columns_to_normalize])\n",
    "    return new_df\n",
    "\n",
    "def clean_data(old_df):\n",
    "    \n",
    "    # Start with a fresh copy of the df\n",
    "    df = old_df.copy()\n",
    "    \n",
    "    # Get adjust term\n",
    "    df = get_term_adj(df)\n",
    "    \n",
    "    # Drop cols with redacted identifying info\n",
    "    columns_to_drop = ['id', 'member_id', 'emp_title']\n",
    "    df = df.drop(columns_to_drop, axis = 1)\n",
    "    \n",
    "    # Cols to keep\n",
    "    cols = ['funded_amnt', 'emp_length', 'home_ownership', 'int_rate', 'purpose', 'total_pymnt',\n",
    "        'annual_inc', 'verification_status', 'dti', 'loan_status', 'revol_util', 'grade', \n",
    "            'term', 'term_adj', 'zip_code']\n",
    "    # Cols to one hot encode\n",
    "    cols_encode = ['emp_length', 'home_ownership', 'verification_status', 'grade', 'purpose']\n",
    "    # Cols to normalize\n",
    "    cols_normalize = ['funded_amnt', 'annual_inc', 'dti']\n",
    "    \n",
    "    ## Apply initial model fn to clean the df\n",
    "    df = initial_model(df, cols, cols_encode, cols_normalize)\n",
    "\n",
    "    # Format using helper fns\n",
    "    df['term'] = df['term'].map(format_term, na_action = 'ignore')\n",
    "    df['int_rate'] = df['int_rate'].map(format_interest, na_action='ignore')\n",
    "    df['revol_util'] = df['revol_util'].map(format_revol, na_action='ignore')\n",
    "    \n",
    "    # Mean imputation for revolving utilization and interest rate\n",
    "    df[df['revol_util'].isnull()] = np.mean(df['revol_util'])\n",
    "    df[df['int_rate'].isnull()] = np.mean(df['int_rate'])\n",
    "    \n",
    "    # Delete any current loans\n",
    "    df = df[df.loan_status != 'Current']\n",
    "    \n",
    "    # Reduce loan status to a flag <-- RESPONSE VAR\n",
    "    df['paid'] = 1;\n",
    "    df.loc[df.loan_status == 'Fully Paid', 'paid'] = 0;\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py:323: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py:323: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "data_train = clean_data(data_train)\n",
    "df_2015 = clean_data(data_2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note: Deleted the 1 row with educational purpose and column for educational purpose\n",
    "df_2015 = df_2015[df_2015.purpose_educational != 1]\n",
    "df_2015 = df_2015.drop(['purpose_educational'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2015.columns == data_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.to_csv('cleaned_2013_14', index = False)\n",
    "df_2015.to_csv('cleaned_2015', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

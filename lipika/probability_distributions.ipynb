{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bhavenpatel/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.models import load_model\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pickle\n",
    "from lightgbm.sklearn import LGBMClassifier\n",
    "\n",
    "## you'll have to pip install LGBM\n",
    "# !pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(\"../lipika/cleaned_2013_14\", low_memory = False);\n",
    "data_test = pd.read_csv(\"../lipika/cleaned_2015\", low_memory = False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = data_test.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.columns == data_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df, cols):\n",
    "    x = df.drop(cols, axis = 1)\n",
    "    y = df.paid\n",
    "    return x, y\n",
    "\n",
    "cols_to_drop_training = ['loan_status', 'paid', 'amnt', 'total_pymnt', 'term_adj', 'zip_code']\n",
    "x_train_initial, y_train_initial = split_data(data_train, cols_to_drop_training)\n",
    "x_test, y_test = split_data(data_test, cols_to_drop_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=1, ratio = 1.0)\n",
    "x_train, y_train = sm.fit_sample(x_train_initial, y_train_initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_test = x_test.dropna()\n",
    "# y_test = y_test.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bhavenpatel/anaconda3/lib/python3.6/site-packages/sklearn/base.py:251: UserWarning: Trying to unpickle estimator PolynomialFeatures from version 0.20.1 when using version 0.20.0. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/Users/bhavenpatel/anaconda3/lib/python3.6/site-packages/sklearn/base.py:251: UserWarning: Trying to unpickle estimator QuadraticDiscriminantAnalysis from version 0.20.1 when using version 0.20.0. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/Users/bhavenpatel/anaconda3/lib/python3.6/site-packages/sklearn/base.py:251: UserWarning: Trying to unpickle estimator Pipeline from version 0.20.1 when using version 0.20.0. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "with open('Tuned_RF.pkl', 'rb') as file:  \n",
    "    rf = pickle.load(file)\n",
    "\n",
    "with open('QDA.pkl', 'rb') as file:  \n",
    "    qda = pickle.load(file, encoding='latin1')\n",
    "\n",
    "# with open('Tuned_LGBM.pkl', 'rb') as file:  \n",
    "#     lgbm = pickle.load(file, encoding='latin1')\n",
    "\n",
    "with open('../anthony/AdaboostGS.pkl', 'rb') as file:  \n",
    "    adaboost = pickle.load(file, encoding='latin1')\n",
    "\n",
    "with open('../bhaven/Tuned_logReg_all_training_data.pkl', 'rb') as file:  \n",
    "    logreg = pickle.load(file, encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = [rf, qda, lgbm, adaboost, logreg]\n",
    "models = [rf, qda, adaboost, logreg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(data_1, data_2, penal = 0.5):\n",
    "    df = pd.DataFrame(data_1)\n",
    "    \n",
    "    df['int_rate'] = data_2['int_rate']\n",
    "    df['amnt'] = data_2['amnt']\n",
    "    df['total_pymnt'] = data_2['total_pymnt']\n",
    "    df['term_adj'] = data_2['term_adj']\n",
    "    df['ROI'] = (((1 + df['int_rate'])*(1-df['proba']))+(df['proba']*drr*penal))-1\n",
    "\n",
    "    df['Real_ROI'] = df['amnt']*(((df['total_pymnt']/df['amnt'])**(1/df['term_adj']))-1)\n",
    "    df['annualized_amnt'] = df['amnt']*(1/df['term_adj'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8999757837386154\n"
     ]
    }
   ],
   "source": [
    "distress = data_train[data_train.paid == 1]\n",
    "drr = (np.sum(distress.total_pymnt)/np.sum(distress.amnt))**(1/np.mean(distress.term))\n",
    "print(drr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('POLY', PolynomialFeatures(degree=2, include_bias=False, interaction_only=False)), ('RFC', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=12, max_features=100, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=No...obs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False))])\n",
      "(346843, 44)\n",
      "346843\n",
      "Pipeline(memory=None,\n",
      "     steps=[('POLY', PolynomialFeatures(degree=2, include_bias=False, interaction_only=False)), ('QDA', QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,\n",
      "               store_covariance=False, store_covariances=None, tol=0.0001))])\n"
     ]
    }
   ],
   "source": [
    "train_data = {}\n",
    "test_data= {}\n",
    "df_train = {}\n",
    "df_test = {}\n",
    "\n",
    "for model in models:\n",
    "    print(model)\n",
    "    tr = {'proba' : model.predict_proba(x_train)[:, 1], \n",
    "                         'pred' : model.predict(x_train), \n",
    "                         'true_val' : y_train}\n",
    "    df_tr = create_df(tr, data_train)\n",
    "    train_data[model] = tr\n",
    "    df_train[model] = df_tr\n",
    "    \n",
    "    print(x_test.shape)\n",
    "    te = {'proba' : model.predict_proba(x_test)[:, 1], \n",
    "                         'pred' : model.predict(x_test), \n",
    "                         'true_val' : y_test}    \n",
    "    print(len(te['proba']))\n",
    "    df_te = create_df(te, data_test)    \n",
    "    test_data[model] = te\n",
    "    df_test[model] = df_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load neural net\n",
    "nn = load_model('NN_final_model.h5')\n",
    "\n",
    "tr = {'proba' : model.predict_proba(x_train)[:, 1], \n",
    "                         'pred' : model.predict(x_train), \n",
    "                         'true_val' : y_train}\n",
    "\n",
    "nn_train = {'proba': nn.predict(x_train)[:, 1], 'true_val': y_train}\n",
    "nn_test = {'proba': nn.predict(x_test)[:, 1], 'true_val': y_test}\n",
    "\n",
    "nn_train['pred'] = np.where(nn_train['proba'] > 0.5, 1, 0)\n",
    "nn_test['pred'] = np.where(nn_test['proba'] > 0.5, 1, 0)\n",
    "\n",
    "df_train['nn'] = create_df(nn_train, data_train)\n",
    "df_test['nn'] =  create_df(nn_test, data_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('df_train_dict.pickle', 'wb') as handle:\n",
    "    pickle.dump(df_train, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('df_test_dict.pickle', 'wb') as handle:\n",
    "    pickle.dump(df_test, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# with open('filename.pickle', 'rb') as handle:\n",
    "#     b = pickle.load(handle)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train_data[model] = {'proba' : model.predict_proba(x_train)[:, 1], 'pred' : model.predict(x_train), 'true_val' : y_train}\n",
    "# # test_data = {'proba' : rf_complex.predict_proba(x_test)[:, 1], 'pred' : rf_complex.predict(x_test),  'true_val' : y_test}\n",
    "\n",
    "# df_train = create_df(train_data, data_train)\n",
    "# df_test =  create_df(test_data, data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline_train = np.mean(df_train.ROI)\n",
    "# baseline_test = np.mean(df_test.ROI)\n",
    "# print(baseline_train, baseline_test)\n",
    "# alpha_low = 0\n",
    "# alpha_high = -np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(roc_auc_score(df_train.true_val, df_train.pred))\n",
    "# print(roc_auc_score(df_test.true_val, df_test.pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.sum(df_train.true_val)/df_train.shape[0])\n",
    "# print(1 - np.sum(df_test.true_val)/df_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loans_to_buy_train = df_train[(df_train.ROI > alpha_low*baseline_train) & (df_train.ROI < alpha_high*baseline_train)]\n",
    "# loans_to_buy_test = df_test[(df_test.ROI > alpha_low*baseline_test) & (df_test.ROI < alpha_high*baseline_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def return_performance(df_sub, df_main):\n",
    "#     perf_subset = np.sum(df_sub.Real_ROI)/np.sum(df_sub.annualized_amnt)\n",
    "#     perf_mainset = np.sum(df_main.Real_ROI)/np.sum(df_main.annualized_amnt)\n",
    "#     return (perf_subset, perf_mainset, perf_subset - perf_mainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_set_performance = return_performance(loans_to_buy_train, df_train)\n",
    "# testing_set_performance = return_performance(loans_to_buy_test, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(15,6));\n",
    "# ax = plt.subplot(1,1,1);\n",
    "\n",
    "# ax.hist(df_test.loc[df_test.true_val == 0,'proba'], density = True, bins = 50, label='Paid Off', alpha=0.6);\n",
    "# ax.hist(df_test.loc[df_test.true_val == 1,'proba'], density = True, bins = 50, label='Defaulted', alpha=0.5);\n",
    "# ax.set_title('Initial Predicted ROI for Defaulted & Non-Defaulted Customers with Higher Order Random Forest')\n",
    "# ax.legend(loc='best')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(15,6));\n",
    "# ax = plt.subplot(1,1,1);\n",
    "\n",
    "# ax.hist(df_train.loc[df_train.true_val == 0,'proba'], bins = 50, label='Paid Off', alpha=0.6);\n",
    "# ax.hist(df_train.loc[df_train.true_val == 1,'proba'], bins = 50, label='Defaulted', alpha=0.5);\n",
    "# ax.set_title('Initial Predicted ROI for Defaulted & Non-Defaulted Customers with Higher Order Random Forest')\n",
    "# ax.legend(loc='best')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.kdeplot(df_train.loc[df_train.true_val == 0,'proba'], label = 'Paid Out');\n",
    "# sns.kdeplot(df_train.loc[df_train.true_val == 1,'proba'], label = 'Defaulted');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set(rc={'figure.figsize':(16,8)})\n",
    "# sns.kdeplot(df_test.loc[df_test.true_val == 0,'proba'], label = 'Paid Out');\n",
    "# sns.kdeplot(df_test.loc[df_test.true_val == 1,'proba'], label = 'Defaulted', linestyle='--');\n",
    "# plt.ylim(0, 3)\n",
    "# plt.xlim(0, 1);\n",
    "# plt.xlabel('Probability of Default', size = 15);\n",
    "# plt.ylabel('Density of Distributions', size = 15);\n",
    "# plt.legend(prop={'size': 13});\n",
    "# plt.savefig('default_probability_distribution.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.kdeplot(df_test['ROI'], label = '2015 Data');\n",
    "# sns.kdeplot(df_train['ROI'], label = '2014 Data', linestyle='--');\n",
    "# plt.ylim(0, 5)\n",
    "# plt.xlim(-0.5, 0.1);\n",
    "# plt.xlabel('Predicted Return on Investment', size = 15);\n",
    "# plt.ylabel('Density of Distributions', size = 15);\n",
    "# plt.legend(prop={'size': 13});\n",
    "# plt.savefig('ROI_distribution.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

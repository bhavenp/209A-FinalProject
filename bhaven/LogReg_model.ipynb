{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training data is from 2013-2014\n",
    "data_train = pd.read_csv(\"../lipika/cleaned_2013_14\", low_memory = False);\n",
    "#test data is from 2015\n",
    "data_test = pd.read_csv(\"../lipika/cleaned_2015\", low_memory = False);\n",
    "data_test = data_test.dropna(); #drop all rows with NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(351757, 50)\n",
      "Index(['funded_amnt', 'int_rate', 'total_pymnt', 'annual_inc', 'dti',\n",
      "       'loan_status', 'revol_util', 'term', 'term_adj', 'zip_code',\n",
      "       'emp_length_1 year', 'emp_length_10+ years', 'emp_length_2 years',\n",
      "       'emp_length_3 years', 'emp_length_4 years', 'emp_length_5 years',\n",
      "       'emp_length_6 years', 'emp_length_7 years', 'emp_length_8 years',\n",
      "       'emp_length_9 years', 'emp_length_< 1 year', 'home_ownership_ANY',\n",
      "       'home_ownership_MORTGAGE', 'home_ownership_OWN', 'home_ownership_RENT',\n",
      "       'verification_status_Not Verified',\n",
      "       'verification_status_Source Verified', 'verification_status_Verified',\n",
      "       'grade_A', 'grade_B', 'grade_C', 'grade_D', 'grade_E', 'grade_F',\n",
      "       'grade_G', 'purpose_car', 'purpose_credit_card',\n",
      "       'purpose_debt_consolidation', 'purpose_home_improvement',\n",
      "       'purpose_house', 'purpose_major_purchase', 'purpose_medical',\n",
      "       'purpose_moving', 'purpose_other', 'purpose_renewable_energy',\n",
      "       'purpose_small_business', 'purpose_vacation', 'purpose_wedding', 'amnt',\n",
      "       'paid'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(data_train.shape)\n",
    "print(data_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check that train and test sets have same columns\n",
    "data_test.columns == data_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(351757, 50)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#only want 10% of the training data to train logistic regression model on\n",
    "train_data_subset, train_data_rest = train_test_split(data_train, test_size=0.0, random_state=42);\n",
    "train_data_subset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into x_train and y_train\n",
    "def split_data(df, cols):\n",
    "    x = df.drop(cols, axis = 1)\n",
    "    y = df.paid\n",
    "    return x, y\n",
    "\n",
    "cols_to_drop_training = ['loan_status', 'paid', 'amnt', 'total_pymnt', 'term_adj', 'zip_code'];\n",
    "#split the training subset\n",
    "x_train_subset, y_train_subset = split_data(train_data_subset, cols_to_drop_training)\n",
    "x_train_rest, y_train_rest = split_data(train_data_rest, cols_to_drop_training)\n",
    "#split test data\n",
    "x_test, y_test = split_data(data_test, cols_to_drop_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(578050, 44)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#upsample default loans (class 1)\n",
    "sm = SMOTE(random_state=42, ratio = 1.0)\n",
    "x_train, y_train = sm.fit_sample(x_train_subset, y_train_subset);\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function to take advantage of sklearn make_pipeline\n",
    "#pipeline adds second order terms and interaction terms to X_train and then fits a Logisitic Regression model\n",
    "\n",
    "# def logreg_pipeline(x, y):\n",
    "#     model = make_pipeline(\n",
    "#     PolynomialFeatures(degree=2, include_bias=False),\n",
    "#     LogisticRegressionCV(cv = 5, penalty = 'l2', max_iter = 2500) );\n",
    "#     model.fit(x, y)\n",
    "#     return model\n",
    "\n",
    "def logreg_pipeline(x, y):\n",
    "    model = make_pipeline(\n",
    "    LogisticRegressionCV(cv = 5, penalty = 'l2', max_iter = 2500) );\n",
    "    model.fit(x, y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train: 11.467177533333333 min\n"
     ]
    }
   ],
   "source": [
    "from time import clock\n",
    "\n",
    "c = clock();\n",
    "logReg_model = logreg_pipeline(x_train, y_train);\n",
    "stop = clock();\n",
    "\n",
    "print(\"Time to train: {0} min\".format( (stop-c)/60) );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the accuracy score of the Logistic Regression model on the rest of the training data\n",
    "# acc_score_rest_training = accuracy_score(y_train_rest, logReg_model.predict(x_train_rest))\n",
    "acc_score_test = accuracy_score(y_test, logReg_model.predict(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.7463203812676052\n"
     ]
    }
   ],
   "source": [
    "# print(\"Validation accuracy:\", acc_score_rest_training)\n",
    "print(\"Test accuracy:\", acc_score_test)\n",
    "#0.6423 accuracy with just main effects and training on 3% of the training data\n",
    "#0.6527 accuracy with just main effects and training on 10% of the training data\n",
    "#0.64940 accuaracy with just main effects and training on 20% of the training data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the logistic regression model on all of the 2013-2014 training data using just main effects and SMOTE (to upsample the default class), I achieve a <75% prediction accuracy on the test set. I was receiving only 65% accuracy when using a validation set from the training data, so this is an intriguing result. When I add any interaction terms or polynomial features, the model fails to converge, so I stuck to just the main effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logReg_pkl_filename = 'Tuned_logReg_all_training_data.pkl'\n",
    "# Open the file to save as pkl file\n",
    "logReg_model_pkl = open(logReg_pkl_filename, 'wb')\n",
    "pickle.dump(logReg_model)\n",
    "# Close the pickle instances\n",
    "logReg_model_pkl.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
